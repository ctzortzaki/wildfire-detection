# -*- coding: utf-8 -*-
"""wildfirePrediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1hhtDGeGdj9ZRg6zapY249O4Z7n5vCpsq
"""

from google.colab import drive
drive.mount('/content/drive')

train_dir = '/content/drive/MyDrive/the_wildfire_dataset_2n_version'
test_dir = '/content/drive/MyDrive/the_wildfire_dataset_2n_version1'

import os
import torch
import torch.nn as nn
import torch.optim as optim
from torchvision import datasets, transforms, models
from torch.utils.data import DataLoader, ConcatDataset, Subset
from sklearn.metrics import classification_report, confusion_matrix
from collections import Counter
import matplotlib.pyplot as plt
import seaborn as sns
import timm

# # === Configuration ===
batch_size = 32
num_epochs = 20
img_size = 224
learning_rate = 1e-3
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(device)

# === Transforms ===
transform_train = transforms.Compose([
    transforms.Resize((img_size, img_size)),
    transforms.RandomHorizontalFlip(),
    transforms.RandomRotation(15),
    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.ToTensor(),
    transforms.Normalize([0.5]*3, [0.5]*3)
])

transform = transforms.Compose([
    transforms.Resize((img_size, img_size)),
    transforms.ToTensor(),
    transforms.Normalize([0.5]*3, [0.5]*3)
])

# === Dataset Loading ===
train_data = datasets.ImageFolder(os.path.join(train_dir, "train"), transform=transform_train)
test_data = datasets.ImageFolder(os.path.join(test_dir, "test"), transform=transform)
val_data = datasets.ImageFolder(os.path.join(test_dir, "val"), transform=transform)

train_loader = DataLoader(train_data, batch_size=32, shuffle=True)
val_loader = DataLoader(val_data, batch_size=32, shuffle=False)
test_loader = DataLoader(test_data, batch_size=32, shuffle=False)

class_names = train_data.classes

def build_model_from_scratch():
    model = models.resnet18(pretrained=False)
    in_feats = model.fc.in_features
    model.fc = nn.Sequential(
        nn.Linear(in_feats, 512),       # project 512â†’512-dim bottleneck
        nn.BatchNorm1d(512),            # stabilize and accelerate training
        nn.ReLU(inplace=True),          # non-linearity
        nn.Dropout(p=0.5),              # drop half-units each batch
        nn.Linear(512, len(class_names))# final logits
    )
    return model

def build_finetuned_model():
    model = models.resnet18(pretrained=True)  # Load pre-trained ResNet18
    for param in model.parameters():
        param.requires_grad = False  # Freeze all layers except the final fully connected layer

    model.fc = nn.Linear(model.fc.in_features, len(class_names))
    return model

def ViT_model():
  model = timm.create_model('vit_base_patch16_224', pretrained=True)
  model.head = nn.Linear(model.head.in_features, len(class_names))
  model = model.to(device)
  return model

def train_model(model, train_loader, val_loader, criterion, optimizer, epochs=10):
    for epoch in range(epochs):
        model.train()
        total_loss = 0
        correct_train = 0
        total_train = 0
        for inputs, labels in train_loader:
            inputs, labels = inputs.to(device), labels.to(device)
            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
            total_loss += loss.item()

            _, preds = torch.max(outputs, 1)
            correct_train += (preds == labels).sum().item()
            total_train += labels.size(0)

        avg_train_loss = total_loss / len(train_loader)
        train_accuracy = correct_train / total_train

        # Validation phase
        model.eval()
        val_loss = 0
        correct_val = 0
        total_val = 0
        with torch.no_grad():
            for inputs, labels in val_loader:
                inputs, labels = inputs.to(device), labels.to(device)
                outputs = model(inputs)
                loss = criterion(outputs, labels)
                val_loss += loss.item()
                _, preds = torch.max(outputs, 1)
                correct_val += (preds == labels).sum().item()
                total_val += labels.size(0)

        avg_val_loss = val_loss / len(val_loader)
        val_accuracy = correct_val / total_val

        print(f"Epoch {epoch+1}/{epochs} | "
              f"Train Loss: {avg_train_loss:.4f}, Train Acc: {train_accuracy:.4f} | "
              f"Val Loss: {avg_val_loss:.4f}, Val Acc: {val_accuracy:.4f}")

def evaluate_model(model, test_loader):
    model.eval()
    y_true, y_pred = [], []
    with torch.no_grad():
        for inputs, labels in test_loader:
            inputs = inputs.to(device)
            outputs = model(inputs)
            preds = outputs.argmax(1).cpu().numpy()
            y_pred.extend(preds)
            y_true.extend(labels.numpy())

    # Metrics
    print("\nClassification Report:")
    print(classification_report(y_true, y_pred, target_names=class_names))
    cm = confusion_matrix(y_true, y_pred)
    print("Confusion Matrix:")
    print(confusion_matrix(y_true, y_pred))


# Calculate class weights to handle class imbalance
# - Counts how many samples belong to each class in the training set
# - Computes inverse frequency for each class to give higher weight to underrepresented class(fire)
"""from collections import Counter
counts = Counter(train_data.targets)
total = sum(counts.values())
class_weights = [total/counts[i] for i in range(len(class_names))]
class_weights = torch.tensor(class_weights, device=device, dtype=torch.float)

# === Train from Scratch ===
print("Training model from scratch...")
model = build_model_from_scratch().to(device)
criterion = nn.CrossEntropyLoss(weight=class_weights)
optimizer = optim.Adam(model.parameters(), lr=learning_rate)"""

# === Train from Scratch ===
print("Training model from scratch...")
model = build_model_from_scratch().to(device)
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=learning_rate)

train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs)
evaluate_model(model, test_loader)

# === Fine-tuning Pretrained Model ===
print("\nTraining model with fine-tuning...")
model = build_finetuned_model().to(device)
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.fc.parameters(), lr=learning_rate)

train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs)
evaluate_model(model, test_loader)

# === Fine-tuning Video Transformer ===
print("\nTraining model with fine-tuning...")
model= ViT_model().to(device)
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.head.parameters(), lr=learning_rate)

train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs)
evaluate_model(model, test_loader)
