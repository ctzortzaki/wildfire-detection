{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"A100","authorship_tag":"ABX9TyMK8C2usofD+2cb+X8Je+so"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"b19e43eb3ada482bb9873833ecca8e5d":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_1e1ebc559d0149eab8aaf950b4308512","IPY_MODEL_a3ceaf2d17fe4f4e819e947300e4b557","IPY_MODEL_8c4174596532456ca0dd45259916b398"],"layout":"IPY_MODEL_34bd4b90fe4d41778e228fbf8b0a38c9"}},"1e1ebc559d0149eab8aaf950b4308512":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_322561f7c3e64c509f74f6a8cec18307","placeholder":"​","style":"IPY_MODEL_236654088ea540aab285dea162d2288b","value":"model.safetensors: 100%"}},"a3ceaf2d17fe4f4e819e947300e4b557":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_0712a0b005b84f32b84a64bd57c98237","max":346284714,"min":0,"orientation":"horizontal","style":"IPY_MODEL_2ed2db144ae844488aae6189d14df17b","value":346284714}},"8c4174596532456ca0dd45259916b398":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a406f5acfe8a4c71b0d54d98bfe55471","placeholder":"​","style":"IPY_MODEL_9b5c4ce7fd0b4c57b4962f6f9a5fd509","value":" 346M/346M [00:01&lt;00:00, 253MB/s]"}},"34bd4b90fe4d41778e228fbf8b0a38c9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"322561f7c3e64c509f74f6a8cec18307":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"236654088ea540aab285dea162d2288b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0712a0b005b84f32b84a64bd57c98237":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2ed2db144ae844488aae6189d14df17b":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"a406f5acfe8a4c71b0d54d98bfe55471":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9b5c4ce7fd0b4c57b4962f6f9a5fd509":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MlOt6LymGcCd","executionInfo":{"status":"ok","timestamp":1750003831238,"user_tz":-180,"elapsed":15586,"user":{"displayName":"Jim Roidis","userId":"06051195264854919978"}},"outputId":"424d87f3-859b-49c5-ac99-87a1400f29c1"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","train_dir = '/content/drive/MyDrive/the_wildfire_dataset_2n_version'\n","test_dir = '/content/drive/MyDrive/the_wildfire_dataset_2n_version'"]},{"cell_type":"code","source":["import os\n","from collections import Counter\n","from torchvision.datasets import ImageFolder\n","\n","# Assume your test images are organized under test_dir/test/<class_name>/\n","test_path = os.path.join(test_dir, 'test')\n","\n","# Use ImageFolder to read class names and image paths\n","test_dataset = ImageFolder(test_path)\n","\n","# Count how many images belong to each class\n","counts = Counter([label for _, label in test_dataset.samples])\n","\n","print(\"Class balance for the test set:\")\n","for idx, class_name in enumerate(test_dataset.classes):\n","    print(f\"  {class_name}: {counts[idx]} images\")"],"metadata":{"id":"tCLFD-29EzsE","executionInfo":{"status":"ok","timestamp":1750003843887,"user_tz":-180,"elapsed":11695,"user":{"displayName":"Jim Roidis","userId":"06051195264854919978"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"f30c38a7-c6e3-4403-a8a3-742a227e75d5"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Class balance for the test set:\n","  fire: 159 images\n","  nofire: 251 images\n"]}]},{"cell_type":"code","source":["import os\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torchvision import datasets, transforms, models\n","from torch.utils.data import DataLoader, ConcatDataset, Subset\n","from sklearn.metrics import classification_report, confusion_matrix\n","from collections import Counter\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import timm\n","\n","# # === Configuration ===\n","# train_dir = '../Project/the_wildfire_dataset_2n_version/train/'\n","# test_dir = '../Project/the_wildfire_dataset_2n_version/test/'\n","batch_size = 32\n","num_epochs = 20\n","img_size = 224\n","learning_rate = 1e-3\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(device)\n","\n","# === Transforms ===\n","transform_train = transforms.Compose([\n","    transforms.Resize((img_size, img_size)),\n","    transforms.RandomHorizontalFlip(),\n","    transforms.RandomRotation(15),\n","    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n","    transforms.ToTensor(),\n","    transforms.Normalize([0.5]*3, [0.5]*3)\n","])\n","\n","transform = transforms.Compose([\n","    transforms.Resize((img_size, img_size)),\n","    transforms.ToTensor(),\n","    transforms.Normalize([0.5]*3, [0.5]*3)\n","])\n","\n","# === Dataset Loading ===\n","train_data = datasets.ImageFolder(os.path.join(train_dir, \"train\"), transform=transform_train)\n","test_data = datasets.ImageFolder(os.path.join(test_dir, \"test\"), transform=transform)\n","val_data = datasets.ImageFolder(os.path.join(test_dir, \"val\"), transform=transform)\n","\n","train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n","val_loader = DataLoader(val_data, batch_size=32, shuffle=False)\n","test_loader = DataLoader(test_data, batch_size=32, shuffle=False)\n","\n","class_names = train_data.classes"],"metadata":{"id":"SGQ_RCUVGw9b","executionInfo":{"status":"ok","timestamp":1750003860824,"user_tz":-180,"elapsed":11017,"user":{"displayName":"Jim Roidis","userId":"06051195264854919978"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"0cb0397d-0833-4027-f7ce-cdb7218bc9c4"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["cuda\n"]}]},{"cell_type":"code","source":["def build_model_from_scratch():\n","    model = models.resnet18(pretrained=False)\n","    model.fc = nn.Linear(model.fc.in_features, len(class_names))\n","    return model\n","\n","\n","\n","def build_finetuned_model():\n","    model = models.resnet18(pretrained=True)  # Load pre-trained ResNet18\n","    for param in model.parameters():\n","        param.requires_grad = False  # Freeze all layers except the final fully connected layer\n","\n","    model.fc = nn.Linear(model.fc.in_features, len(class_names))\n","    return model\n","\n","def ViT_model():\n","  model = timm.create_model('vit_base_patch16_224', pretrained=True)\n","  model.head = nn.Linear(model.head.in_features, len(class_names))\n","  model = model.to(device)\n","  return model"],"metadata":{"id":"GUBhGwpmGx9y","executionInfo":{"status":"ok","timestamp":1750003867108,"user_tz":-180,"elapsed":8,"user":{"displayName":"Jim Roidis","userId":"06051195264854919978"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["# def train_model(model, train_loader, val_loader, epochs=10):\n","def train_model(model, train_loader, val_loader, criterion, optimizer, epochs=10):\n","    for epoch in range(epochs):\n","        model.train()\n","        total_loss = 0\n","        correct_train = 0\n","        total_train = 0\n","        for inputs, labels in train_loader:\n","            inputs, labels = inputs.to(device), labels.to(device)\n","            optimizer.zero_grad()\n","            outputs = model(inputs)\n","            loss = criterion(outputs, labels)\n","            loss.backward()\n","            optimizer.step()\n","            total_loss += loss.item()\n","\n","            _, preds = torch.max(outputs, 1)\n","            correct_train += (preds == labels).sum().item()\n","            total_train += labels.size(0)\n","\n","        avg_train_loss = total_loss / len(train_loader)\n","        train_accuracy = correct_train / total_train\n","\n","        # Validation phase\n","        model.eval()\n","        val_loss = 0\n","        correct_val = 0\n","        total_val = 0\n","        with torch.no_grad():\n","            for inputs, labels in val_loader:\n","                inputs, labels = inputs.to(device), labels.to(device)\n","                outputs = model(inputs)\n","                loss = criterion(outputs, labels)\n","                val_loss += loss.item()\n","                _, preds = torch.max(outputs, 1)\n","                correct_val += (preds == labels).sum().item()\n","                total_val += labels.size(0)\n","\n","        avg_val_loss = val_loss / len(val_loader)\n","        val_accuracy = correct_val / total_val\n","\n","        print(f\"Epoch {epoch+1}/{epochs} | \"\n","              f\"Train Loss: {avg_train_loss:.4f}, Train Acc: {train_accuracy:.4f} | \"\n","              f\"Val Loss: {avg_val_loss:.4f}, Val Acc: {val_accuracy:.4f}\")"],"metadata":{"id":"0OMIySQVG2fo","executionInfo":{"status":"ok","timestamp":1750003868676,"user_tz":-180,"elapsed":11,"user":{"displayName":"Jim Roidis","userId":"06051195264854919978"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["def evaluate_model(model, test_loader):\n","    model.eval()\n","    y_true, y_pred = [], []\n","    with torch.no_grad():\n","        for inputs, labels in test_loader:\n","            inputs = inputs.to(device)\n","            outputs = model(inputs)\n","            preds = outputs.argmax(1).cpu().numpy()\n","            y_pred.extend(preds)\n","            y_true.extend(labels.numpy())\n","\n","    # Metrics\n","    print(\"\\nClassification Report:\")\n","    print(classification_report(y_true, y_pred, target_names=class_names))\n","    cm = confusion_matrix(y_true, y_pred)\n","    print(\"Confusion Matrix:\")\n","    print(confusion_matrix(y_true, y_pred))"],"metadata":{"id":"_hqxGUNiG7N8","executionInfo":{"status":"ok","timestamp":1750003872292,"user_tz":-180,"elapsed":5,"user":{"displayName":"Jim Roidis","userId":"06051195264854919978"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["from collections import Counter\n","counts = Counter(train_data.targets)\n","total = sum(counts.values())\n","class_weights = [total/counts[i] for i in range(len(class_names))]\n","class_weights = torch.tensor(class_weights, device=device, dtype=torch.float)\n","\n","# === Train from Scratch ===\n","print(\"Training model from scratch...\")\n","model = build_model_from_scratch().to(device)\n","criterion = nn.CrossEntropyLoss(weight=class_weights)\n","optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n","\n","train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs)\n","evaluate_model(model, test_loader)"],"metadata":{"id":"goyRzFZFG8Af","executionInfo":{"status":"ok","timestamp":1749988359779,"user_tz":-180,"elapsed":14494247,"user":{"displayName":"Jim Roidis","userId":"06051195264854919978"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"2f6b0a43-04d4-4c2c-fc13-359cbac4c533"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["Training model from scratch...\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/PIL/Image.py:3442: DecompressionBombWarning: Image size (89747104 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/PIL/Image.py:3442: DecompressionBombWarning: Image size (104688771 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/20 | Train Loss: 0.7309, Train Acc: 0.6322 | Val Loss: 0.6235, Val Acc: 0.6891\n","Epoch 2/20 | Train Loss: 0.5861, Train Acc: 0.6751 | Val Loss: 0.7506, Val Acc: 0.5448\n","Epoch 3/20 | Train Loss: 0.5793, Train Acc: 0.6725 | Val Loss: 0.8057, Val Acc: 0.4502\n","Epoch 4/20 | Train Loss: 0.5645, Train Acc: 0.6953 | Val Loss: 0.7716, Val Acc: 0.5448\n","Epoch 5/20 | Train Loss: 0.5615, Train Acc: 0.6932 | Val Loss: 0.6482, Val Acc: 0.6393\n","Epoch 6/20 | Train Loss: 0.5613, Train Acc: 0.7011 | Val Loss: 0.6843, Val Acc: 0.6095\n","Epoch 7/20 | Train Loss: 0.5413, Train Acc: 0.7149 | Val Loss: 0.6266, Val Acc: 0.6542\n","Epoch 8/20 | Train Loss: 0.5321, Train Acc: 0.7170 | Val Loss: 0.5977, Val Acc: 0.7040\n","Epoch 9/20 | Train Loss: 0.5089, Train Acc: 0.7424 | Val Loss: 0.6354, Val Acc: 0.6567\n","Epoch 10/20 | Train Loss: 0.5146, Train Acc: 0.7446 | Val Loss: 0.5557, Val Acc: 0.7090\n","Epoch 11/20 | Train Loss: 0.5050, Train Acc: 0.7435 | Val Loss: 0.5796, Val Acc: 0.6965\n","Epoch 12/20 | Train Loss: 0.4797, Train Acc: 0.7689 | Val Loss: 0.4762, Val Acc: 0.7413\n","Epoch 13/20 | Train Loss: 0.4614, Train Acc: 0.7764 | Val Loss: 0.4100, Val Acc: 0.8284\n","Epoch 14/20 | Train Loss: 0.4564, Train Acc: 0.7854 | Val Loss: 0.4453, Val Acc: 0.7886\n","Epoch 15/20 | Train Loss: 0.4466, Train Acc: 0.7880 | Val Loss: 0.4199, Val Acc: 0.8035\n","Epoch 16/20 | Train Loss: 0.4289, Train Acc: 0.8039 | Val Loss: 0.3970, Val Acc: 0.8010\n","Epoch 17/20 | Train Loss: 0.4231, Train Acc: 0.8045 | Val Loss: 0.4220, Val Acc: 0.8060\n","Epoch 18/20 | Train Loss: 0.4296, Train Acc: 0.8050 | Val Loss: 0.4242, Val Acc: 0.8209\n","Epoch 19/20 | Train Loss: 0.4241, Train Acc: 0.8050 | Val Loss: 0.3916, Val Acc: 0.8259\n","Epoch 20/20 | Train Loss: 0.4021, Train Acc: 0.8193 | Val Loss: 0.4448, Val Acc: 0.7861\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/PIL/Image.py:3442: DecompressionBombWarning: Image size (96631920 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/PIL/Image.py:3442: DecompressionBombWarning: Image size (94487082 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/PIL/Image.py:3442: DecompressionBombWarning: Image size (101859328 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","        fire       0.72      0.77      0.75       159\n","      nofire       0.85      0.81      0.83       251\n","\n","    accuracy                           0.80       410\n","   macro avg       0.78      0.79      0.79       410\n","weighted avg       0.80      0.80      0.80       410\n","\n","Confusion Matrix:\n","[[123  36]\n"," [ 48 203]]\n"]}]},{"cell_type":"code","source":["# === Fine-tuning Pretrained Model ===\n","print(\"\\nTraining model with fine-tuning...\")\n","model = build_finetuned_model().to(device)\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.fc.parameters(), lr=learning_rate)  # Fine-tune only the last layer\n","\n","train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs)\n","evaluate_model(model, test_loader)"],"metadata":{"id":"vXxKq3NeHAPE","colab":{"base_uri":"https://localhost:8080/"},"outputId":"d2bb5bb9-a6dc-4d5c-8c18-f7758c784184","executionInfo":{"status":"ok","timestamp":1750002024098,"user_tz":-180,"elapsed":13664312,"user":{"displayName":"Jim Roidis","userId":"06051195264854919978"}}},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Training model with fine-tuning...\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n","Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n","100%|██████████| 44.7M/44.7M [00:00<00:00, 175MB/s]\n","/usr/local/lib/python3.11/dist-packages/PIL/Image.py:3442: DecompressionBombWarning: Image size (104688771 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/PIL/Image.py:3442: DecompressionBombWarning: Image size (89747104 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/20 | Train Loss: 0.5505, Train Acc: 0.7175 | Val Loss: 0.4536, Val Acc: 0.7886\n","Epoch 2/20 | Train Loss: 0.4339, Train Acc: 0.7901 | Val Loss: 0.4299, Val Acc: 0.7861\n","Epoch 3/20 | Train Loss: 0.3947, Train Acc: 0.8278 | Val Loss: 0.4013, Val Acc: 0.8010\n","Epoch 4/20 | Train Loss: 0.3798, Train Acc: 0.8288 | Val Loss: 0.4617, Val Acc: 0.7886\n","Epoch 5/20 | Train Loss: 0.3957, Train Acc: 0.8145 | Val Loss: 0.3889, Val Acc: 0.8159\n","Epoch 6/20 | Train Loss: 0.3816, Train Acc: 0.8246 | Val Loss: 0.3916, Val Acc: 0.8209\n","Epoch 7/20 | Train Loss: 0.3581, Train Acc: 0.8400 | Val Loss: 0.3722, Val Acc: 0.8259\n","Epoch 8/20 | Train Loss: 0.3518, Train Acc: 0.8474 | Val Loss: 0.4136, Val Acc: 0.8159\n","Epoch 9/20 | Train Loss: 0.3591, Train Acc: 0.8426 | Val Loss: 0.3834, Val Acc: 0.8184\n","Epoch 10/20 | Train Loss: 0.3592, Train Acc: 0.8320 | Val Loss: 0.3805, Val Acc: 0.8234\n","Epoch 11/20 | Train Loss: 0.3447, Train Acc: 0.8442 | Val Loss: 0.3658, Val Acc: 0.8159\n","Epoch 12/20 | Train Loss: 0.3290, Train Acc: 0.8633 | Val Loss: 0.3856, Val Acc: 0.8134\n","Epoch 13/20 | Train Loss: 0.3254, Train Acc: 0.8527 | Val Loss: 0.4111, Val Acc: 0.8134\n","Epoch 14/20 | Train Loss: 0.3210, Train Acc: 0.8574 | Val Loss: 0.3568, Val Acc: 0.8333\n","Epoch 15/20 | Train Loss: 0.3230, Train Acc: 0.8612 | Val Loss: 0.3549, Val Acc: 0.8284\n","Epoch 16/20 | Train Loss: 0.3282, Train Acc: 0.8596 | Val Loss: 0.3594, Val Acc: 0.8308\n","Epoch 17/20 | Train Loss: 0.3142, Train Acc: 0.8638 | Val Loss: 0.4830, Val Acc: 0.7886\n","Epoch 18/20 | Train Loss: 0.3441, Train Acc: 0.8590 | Val Loss: 0.3630, Val Acc: 0.8259\n","Epoch 19/20 | Train Loss: 0.3168, Train Acc: 0.8601 | Val Loss: 0.3555, Val Acc: 0.8458\n","Epoch 20/20 | Train Loss: 0.3111, Train Acc: 0.8691 | Val Loss: 0.3689, Val Acc: 0.8408\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/PIL/Image.py:3442: DecompressionBombWarning: Image size (96631920 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/PIL/Image.py:3442: DecompressionBombWarning: Image size (94487082 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/PIL/Image.py:3442: DecompressionBombWarning: Image size (101859328 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","        fire       0.82      0.89      0.85       159\n","      nofire       0.92      0.88      0.90       251\n","\n","    accuracy                           0.88       410\n","   macro avg       0.87      0.88      0.88       410\n","weighted avg       0.89      0.88      0.88       410\n","\n","Confusion Matrix:\n","[[141  18]\n"," [ 30 221]]\n"]}]},{"cell_type":"code","source":["# === Fine-tuning Video Transformer ===\n","print(\"\\nTraining model with fine-tuning...\")\n","model= ViT_model().to(device)\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.head.parameters(), lr=learning_rate)\n","\n","train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs)\n","evaluate_model(model, test_loader)"],"metadata":{"id":"OLefTF9OHCml","colab":{"base_uri":"https://localhost:8080/","height":542,"referenced_widgets":["b19e43eb3ada482bb9873833ecca8e5d","1e1ebc559d0149eab8aaf950b4308512","a3ceaf2d17fe4f4e819e947300e4b557","8c4174596532456ca0dd45259916b398","34bd4b90fe4d41778e228fbf8b0a38c9","322561f7c3e64c509f74f6a8cec18307","236654088ea540aab285dea162d2288b","0712a0b005b84f32b84a64bd57c98237","2ed2db144ae844488aae6189d14df17b","a406f5acfe8a4c71b0d54d98bfe55471","9b5c4ce7fd0b4c57b4962f6f9a5fd509"]},"executionInfo":{"status":"error","timestamp":1750003950529,"user_tz":-180,"elapsed":71029,"user":{"displayName":"Jim Roidis","userId":"06051195264854919978"}},"outputId":"2cb87527-d8a1-4c1d-c0cb-81e2ea5b7a7b"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Training model with fine-tuning...\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["model.safetensors:   0%|          | 0.00/346M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b19e43eb3ada482bb9873833ecca8e5d"}},"metadata":{}},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-9-1856700957>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mevaluate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-7-609336400>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, val_loader, criterion, optimizer, epochs)\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mcorrect_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mtotal_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m             \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 708\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    709\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m             if (\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    762\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    763\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 764\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    765\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    243\u001b[0m         \"\"\"\n\u001b[1;32m    244\u001b[0m         \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 245\u001b[0;31m         \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m             \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mdefault_loader\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    282\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0maccimage_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 284\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mpil_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    285\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mpil_loader\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    261\u001b[0m     \u001b[0;31m# open path as file to avoid ResourceWarning (https://github.com/python-pillow/Pillow/issues/835)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 263\u001b[0;31m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    264\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"RGB\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   3514\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3515\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3516\u001b[0;31m     \u001b[0mprefix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3517\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3518\u001b[0m     \u001b[0mpreinit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]}]}